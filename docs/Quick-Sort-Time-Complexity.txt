Quick sort Time complexity
==================================

n = 50,000
average_case = 0m0.020s, 0m0.021s
worst_case = 0m1.999s, 0m2.016s
----------------------------------

n = 100,000
average_case = 0m0.034s, 0m0.041s
worst_case = 0m7.922s, 0m8.928s
----------------------------------

n = 150,000
average_case = 0m0.049s, 0m0.051s
worst_case = 0m18.261s, 0m18.813s
----------------------------------

n = 200,000
average_case = 0m0.060s, 0m0.062s
worst_case = 0m32.432s, 0m34.139s
----------------------------------

n = 250,000
average_case = 0m0.071s, 0m0.073s
worst_case = 0m51.916s, 0m52.103s
----------------------------------

n = 300,000
average_case = 0m0.088s, 0m0.089s
worst_case = <segmentation fault>
----------------------------------

n = 5,000,000
average_case = 0m1.298s, 0m1.303s
worst_case = <segmentation fault>
----------------------------------

Possible?
==================================
https://www.wolframalpha.com/input/?i=plot+n%5E2+and+n*log+n+where+n+from+50000+to+250000

doubling the input almost quadruples the time in worst case
all worst_case scenarios were ran on Descending ordered array

Theoretically for n^2 algorithms, with,
0.8 million input, the system performs 6.4*10^11 operations.
1.6 million input, the system performs 25.6*10^11 operations.

This means doubling the input quadruples the number of operations.
